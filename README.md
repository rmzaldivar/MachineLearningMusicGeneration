# Machine Learning Music Generation
This repo aims to build on and alter the methods created in the MusicLM paper by Google Research https://arxiv.org/abs/2301.11325. The goal is to create a text-to-audio machine learning model which as input takes a description of a custom song designed by the user and outputs a corresponding music file of that song as interpreted by the model.

__*Note: In the directory dedicated to each respective part there is another README file with a display that contains a UML diagram which is expected to be updated to represent the model given the git branch you're on, i.e, be cool ᕙ(▀̿̿Ĺ̯̿̿▀̿ ̿)ᕗ please update the diagram if you plan to work on it for the sanity of everyone else <3*__


## |   Installation and Running Tests
*coming soon*

## |   Base Model Components
There are three main components of MusicLM that will need to be generalized. The following three sections are titled as the functions of those components and what we’ll call them in the codebase. Each section details MusicLM’s method of accomplishing it and ours. Each section is designed to be swapped out and improved in parallel by repo contributors without impact to the overall function of the model by encapsulating each part that is described in the following three sections.

### Music Encoder

*coming soon*

### Text Encoder

*coming soon*

### Joint Text+Audio Embedder

*coming soon*

## |   Model Interface Components
These are just two components that will handle the input and output to the model components such that all parts can be worked on in parallel by different repo contributors and general ease of use for repo *enthusiasts*.

### Input Handler

*coming soon*

### Output Handler

*coming soon*

## |   Overall Structure
The following diagrams 

